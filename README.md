# RAG ChatBot - PDF Question Answering System

A Retrieval-Augmented Generation (RAG) chatbot that answers questions based exclusively on the content of a PDF document. Built with Streamlit, LangChain, and Groq API.

## ğŸ¯ What Does This Agent Do?

This chatbot:
- **Reads and understands PDF documents** using advanced text processing
- **Answers questions ONLY from the PDF content** - it won't use outside knowledge
- **Retrieves relevant sections** from the document to provide accurate answers
- **Maintains conversation history** for a seamless chat experience
- **Refuses to answer** questions not covered in the PDF to prevent hallucinations

## âœ¨ Features

- ğŸ“„ PDF document processing and indexing
- ğŸ” Semantic search using FAISS vector store
- ğŸ’¬ Interactive chat interface with conversation history
- ğŸš« Strict context-based responses (no external knowledge)
- âš¡ Fast responses using Groq's LLM API

## ğŸ› ï¸ Technology Stack

- **Streamlit** - Web interface
- **LangChain** - RAG framework
- **FAISS** - Vector database for semantic search
- **HuggingFace Embeddings** - Text embeddings (all-MiniLM-L12-v2)
- **Groq API** - LLM inference (llama-3.1-8b-instant)
- **PyPDF** - PDF processing

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- Groq API key ([Get it here](https://console.groq.com/))

## ğŸš€ Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
```

2. **Install required packages**
```bash
pip install streamlit langchain langchain-groq langchain-community
pip install faiss-cpu sentence-transformers pypdf python-dotenv
```

3. **Set up environment variables**

Create a `.env` file in the project root:
```env
GROQ_API_KEY=your_groq_api_key_here
```

4. **Update PDF path**

In the code, change the PDF file path to your document:
```python
pdf_file = "/path/to/your/document.pdf"
```

## ğŸ® Usage

1. **Run the application**
```bash
streamlit run app.py
```

2. **Access the app**
- Open your browser and go to `http://localhost:8501`

3. **Start chatting**
- Type your question in the chat input
- The bot will search the PDF and provide answers based on the document content
- If your question isn't covered in the PDF, it will inform you

## ğŸ“ Example Questions

Assuming your PDF is about a research paper:
- âœ… "What is the main conclusion of this paper?"
- âœ… "Explain the methodology used in section 3"
- âœ… "What are the key findings?"
- âŒ "What's the weather today?" (Will be refused - not in PDF)

## ğŸ”§ Configuration

You can customize the following parameters in the code:

- **Chunk size**: `chunk_size=1000` - Size of text chunks for processing
- **Chunk overlap**: `chunk_overlap=100` - Overlap between chunks
- **Number of retrieved chunks**: `search_kwargs={'k': 3}` - Number of relevant sections to retrieve
- **Embedding model**: `model_name="all-MiniLM-L12-v2"` - HuggingFace embedding model
- **LLM model**: `model="llama-3.1-8b-instant"` - Groq model

## ğŸ“‚ Project Structure

```
rag-chatbot/
â”‚
â”œâ”€â”€ rag-pdf.py                 # Main application file
â”œâ”€â”€ .env                   # Environment variables (not committed)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ“¦ Requirements.txt

```
streamlit
langchain
langchain-groq
langchain-community
faiss-cpu
sentence-transformers
pypdf
python-dotenv
```



## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¤ Author

Your Name - [(https://github.com/yourusername)](https://github.com/ArslanFarooq3715)

## ğŸ™ Acknowledgments

- LangChain for the RAG framework
- Groq for the fast LLM API
- Streamlit for the web interface
- HuggingFace for the embedding models

---

**Note**: This chatbot is designed for educational and research purposes. Always verify critical information from the original source documents.
